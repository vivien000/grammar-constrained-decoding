{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MlECyS60-oC",
    "outputId": "75905f2a-da61-46cc-c0a4-257a28ccd56f"
   },
   "outputs": [],
   "source": [
    "hf_token = \"xxx\" # HuggingFace token to load the `mistralai/Mistral-7B-v0.1`tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import io\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import functools\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import pygtrie\n",
    "\n",
    "from pyformlang.cfg.variable import Variable\n",
    "from pyformlang.cfg.terminal import Terminal\n",
    "from pyformlang.cfg.production import Production\n",
    "from pyformlang.cfg.cfg import CFG\n",
    "from pyformlang.regular_expression import Regex\n",
    "from pyformlang.finite_automaton.deterministic_finite_automaton import DeterministicFiniteAutomaton as DFA\n",
    "from pyformlang.finite_automaton import EpsilonNFA\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import to_agraph\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3eeDlsVn8jz"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "fb51c671b2cc4bd0a830f17700a1847a",
      "7fb46faf07f14dcc8a6d1106ad3f3604",
      "75d20873f81443f2b25cb7c4b03dc98e",
      "07e085b049df40759fcc501206fe6d38",
      "d7c9a06cf4744ee2a4515a85417295b5",
      "6c73536c2b334371bdabd736bed98802",
      "3660effca87e486ab420ffdfe78c35cb",
      "936defb9ad4a4157b703679ea4b10e0c",
      "17799bfa4c0d45a793f278326a06405c",
      "6968b3ebef17477f98e7671c6e77c13c",
      "322ca904cad5403c8ff1555b10a1daf9",
      "2c3a889ec0cf4ef1b13eea811d736465",
      "2dba1fb04aa04ca09bd7c09f22401511",
      "cb78171d32514589b740ca711d1a275e",
      "51d53428f2004238975b5607550dbdd8",
      "7ec2033100874395aa0fc9cbf822779c",
      "4af153566d4c44d8a66437feb777b696",
      "da31d7ccc21a49e58d67657a90198e75",
      "db5d2e9af0ec45749a426e3a746f4837",
      "355ad2a5c97e453b9d456a99adb00fda",
      "72e322611d8049a4b916148f1f2459b9",
      "08b990d44bdb47ecb51eff0d3b442bf3",
      "f2ef4327e4024bf2935f1ed135b03de5",
      "a2f48961ad3c44aea4ba8a753da6afab",
      "90b07bad47b44a378f8f5d4e7299d452",
      "46634823d9d14de18264eba6175f8307",
      "ecb666e96f314cccbb323abb7a80bce7",
      "8ad2fe47632447d0aae446dbe7fa6bf8",
      "d6e327ad2a3c43d2b031062a287b9b21",
      "63ef3900544f4ee29050d0a47acd7819",
      "fc3d4feaabb44a7ba89ef277724b932a",
      "8b0f358f67714d138637327ce0728c14",
      "3dc769136b534ecf93142ae7e0ec7ec0",
      "6a23b1837fac4d8d9fd1a33abdeb7643",
      "8cb8f7f4ae5648df9a1f50c15d90c9ec",
      "3b464f0d253341d58674803ecbd4ba28",
      "922679c1d4be478aaef7b20f4c5bb522",
      "20a3879179384546888366c51c9fb629",
      "391ea20aa58d4f739e8b1ad63c8eed79",
      "55a8ce618602446f9f72e3b2161467a0",
      "bc0378cfe8b24813949b1a6cea57844b",
      "72aa012d15a444d68602da1fe55978af",
      "a95c7dbf6697497a874449cfb471b4de",
      "e245e4337a074ca0a1fc54f5d2c9c803"
     ]
    },
    "id": "tNhekogo1FnL",
    "outputId": "8910215b-8798-4e63-a2a0-bffd18f8ea3d"
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "def get_token_id(token, tokenizer):\n",
    "    \"\"\"\n",
    "    Return the token id of a given token.\n",
    "    \"\"\"\n",
    "    return tokenizer.backend_tokenizer.model.token_to_id(token)\n",
    "\n",
    "def get_token(token_id, tokenizer):\n",
    "    \"\"\"\n",
    "    Return the token corresponding to a token id.\n",
    "    \"\"\"\n",
    "    token = tokenizer.backend_tokenizer.model.id_to_token(token_id)\n",
    "    if token == \"<0x0A>\":\n",
    "        return \"\\n\"\n",
    "    return token\n",
    "\n",
    "def encode(s, tokenizer):\n",
    "    \"\"\"\n",
    "    Encode a string with a tokenizer.\n",
    "    \"\"\"\n",
    "    # The difference with the standard encode function is that we remove the\n",
    "    # added blank space at the beginning of the string\n",
    "    normalized = tokenizer.backend_tokenizer.normalizer.normalize_str(s)[1:]\n",
    "    return [\n",
    "        x.as_tuple()[0]\n",
    "        for x in tokenizer.backend_tokenizer.model.tokenize(normalized)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZQAzQkR4zwS",
    "outputId": "000eb002-89fc-4025-f57a-94f1ced14560"
   },
   "outputs": [],
   "source": [
    "# We restrict ourselves to ASCII characters\n",
    "\n",
    "characters = {chr(i) for i in range(32, 127)}\n",
    "characters.add(\" \")\n",
    "characters.add(\"\\n\")\n",
    "\n",
    "vocabulary = {}\n",
    "tokenizer_vocab = tokenizer.vocab\n",
    "for w in tokenizer_vocab:\n",
    "    if w == \"<0x0A>\":\n",
    "        vocabulary[\"\\n\"] = tokenizer_vocab[w]\n",
    "        continue\n",
    "    for c in w:\n",
    "        if c not in characters and c != \"▁\":\n",
    "            break\n",
    "    if w.startswith(\"<0x\"):\n",
    "        continue\n",
    "    else:\n",
    "        vocabulary[w.replace(\"▁\", \" \")] = tokenizer_vocab[w]\n",
    "\n",
    "# We create a trie corresponding to the available tokens to accelerate the creation of the token-level NFA.\n",
    "        \n",
    "trie = pygtrie.CharTrie()\n",
    "for t in vocabulary:\n",
    "    trie[t] = vocabulary[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dkE8quWoDzF"
   },
   "source": [
    "# Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "v7cYO9Im98b7"
   },
   "outputs": [],
   "source": [
    "import lark\n",
    "from lark.indenter import PythonIndenter\n",
    "import interegular\n",
    "import re\n",
    "\n",
    "# In the context of this notebook, we focus on the Python grammar, as specified in `lark`.\n",
    "\n",
    "parser = lark.Lark.open(\n",
    "    'python.lark',\n",
    "    parser='lalr',\n",
    "    lexer='basic',\n",
    "    postlex=PythonIndenter(),\n",
    "    start='file_input'\n",
    ")\n",
    "\n",
    "terminals = set([x.name for x in parser.lexer_conf.terminals])\n",
    "non_terminals = set()\n",
    "\n",
    "def get_name(non_terminal):\n",
    "    try:\n",
    "        return non_terminal.fullrepr.split(\"'\")[3]\n",
    "    except IndexError:\n",
    "        return non_terminal.name\n",
    "\n",
    "for rule in parser.rules:\n",
    "    name = get_name(rule.origin)\n",
    "    non_terminals.add(name)\n",
    "    for x in rule.expansion:\n",
    "        if type(x) == lark.grammar.NonTerminal:\n",
    "            non_terminals.add(x.name)\n",
    "        elif type(x) == lark.grammar.Terminal:\n",
    "            terminals.add(x.name)\n",
    "\n",
    "print(len(parser.rules), \"rules,\", len(terminals), \"terminals,\", len(non_terminals), \"non-terminals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "81y89yOeXagV"
   },
   "outputs": [],
   "source": [
    "# Compute the immediate terminal or non-terminal successors of a symbol\n",
    "\n",
    "successors = defaultdict(set)\n",
    "\n",
    "def get_name(non_terminal):\n",
    "    try:\n",
    "        return non_terminal.fullrepr.split(\"'\")[3]\n",
    "    except IndexError:\n",
    "        return non_terminal.name\n",
    "\n",
    "def is_terminal(node):\n",
    "    return node != node.lower()\n",
    "\n",
    "for r in parser.rules:\n",
    "    name = get_name(r.origin)\n",
    "    current = f\"{name}_start\"\n",
    "    for node in r.expansion:\n",
    "        if isinstance(node, lark.grammar.NonTerminal):\n",
    "            new_name = get_name(node)\n",
    "            next = f\"{new_name}_start\"\n",
    "            new_current = f\"{new_name}_end\"\n",
    "        else:\n",
    "            next, new_current = node.name, node.name\n",
    "        successors[current].add(next)\n",
    "        current = new_current\n",
    "    successors[current].add(f\"{name}_end\")\n",
    "\n",
    "# Compute the immediate terminal successors of a symbol\n",
    "\n",
    "terminal_successors = defaultdict(dict)\n",
    "\n",
    "for t in successors:\n",
    "    already_seen = {}\n",
    "    queue = [(str(t), ())]\n",
    "    while len(queue) > 0:\n",
    "        (node, path) = queue.pop()\n",
    "        if node in successors:\n",
    "            for next_node in successors[node]:\n",
    "                if next_node not in already_seen:\n",
    "                    if is_terminal(next_node):\n",
    "                        terminal_successors[t][next_node] = path + (node,)\n",
    "                    else:\n",
    "                        queue.append((next_node, path + (node,)))\n",
    "                        already_seen[next_node] = True\n",
    "\n",
    "# Store the priorities of the terminals\n",
    "\n",
    "terminal2priority = {\n",
    "    v.name: k\n",
    "    for k, v in enumerate(parser.parser.lexer.lexer.terminals)\n",
    "}\n",
    "terminal2priority[\"\"] = -1\n",
    "terminal2priority[\"UNDERSCORE\"] = 1\n",
    "terminal2priority[\"UNDERSCORE\"] = 1\n",
    "terminal2priority[\"_DEDENT\"] = 1\n",
    "terminal2priority[\"_INDENT\"] = 1\n",
    "\n",
    "# Slightly adjust the regex of the terminals so that they are compatible with interegular\n",
    "\n",
    "terminal2regex = {t.name: t.pattern.to_regexp() for t in parser.lexer_conf.terminals}\n",
    "terminal2regex[\"STRING\"] = r\"\"\"([ubfr]|ur|br|rb|fr|rf)?(?:'[^'\\\\\\n]*(?:\\\\.[^'\\\\\\n]*)*'|\"[^\"\\\\\\n]*(?:\\\\.[^\"\\\\\\n]*)*\")\"\"\"\n",
    "terminal2regex[\"LONG_STRING\"] =  r'''(?:(?:[ubf]?[rR]?|[rR]?[ubf]?)(?:(?:\"\"\"(?:[^\\\\]|\\\\.|\\\\\\n)*?\"\"\"''' + r\"\"\"|'''(?:[^\\\\]|\\\\.|\\\\\\n)*?''')))\"\"\"\n",
    "terminal2regex[\"DEC_NUMBER\"] = r\"0|(?:[1-9](?:(?:[0-9])|(?:[0-9]\\_[0-9]))*)\"\n",
    "terminal2regex[\"_DEDENT\"] = \"\"\n",
    "terminal2regex[\"_INDENT\"] = \"\"\n",
    "\n",
    "# Identify and display sets of terminals that are mutually interchangeable\n",
    "\n",
    "terminal2rules = defaultdict(set)\n",
    "\n",
    "for r in parser.rules:\n",
    "    name = r.origin.name if type(r.origin.name) == str else r.origin.name.value\n",
    "    symbols = [x.name for x in r.expansion]\n",
    "    for i in range(len(symbols)):\n",
    "        symbol = symbols[i]\n",
    "        if symbol == symbol.upper():\n",
    "            terminal2rules[symbol].add(name + \":\" + \"-\".join(symbols[:i] + [\"X\"] + symbols[i+1:]))\n",
    "\n",
    "equivalent_terminals = {}\n",
    "for t in terminal2rules:\n",
    "    equivalent_terminals[t] = []\n",
    "    for t2 in terminal2rules:\n",
    "        if terminal2rules[t] == terminal2rules[t2]:\n",
    "            equivalent_terminals[t].append(t2)\n",
    "terminal_replacement = {t: sorted(equivalent_terminals[t])[0] for t in equivalent_terminals}\n",
    "terminals_to_replace = {t for t in terminal_replacement if terminal_replacement[t] != t}\n",
    "\n",
    "def get_regex(t):\n",
    "    if t.startswith(\"__ANON_\"):\n",
    "        return terminal2regex[t].replace(\"\\\\\", \"\")\n",
    "    return t\n",
    "\n",
    "equivalence_classes = defaultdict(set)\n",
    "for t in terminal_replacement:\n",
    "    equivalence_classes[terminal_replacement[t]].add(t)\n",
    "print(\"Sets of interchangeable terminals:\")\n",
    "for c in equivalence_classes:\n",
    "    if len(equivalence_classes[c]) > 1:\n",
    "        print(\"  \"+\", \".join([get_regex(t) for t in equivalence_classes[c]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVQTFtymoGsL"
   },
   "source": [
    "# Character-level NFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "-Ie4xnpguY0o"
   },
   "outputs": [],
   "source": [
    "class CharacterNFA():\n",
    "    \"\"\"\n",
    "    Class to define an NFA at the character level.\n",
    "    \"\"\"\n",
    "    def __init__(self, states, alphabet, map, initial_state, final_states):\n",
    "        self.states = states\n",
    "        self.alphabet = alphabet\n",
    "        self.map = map\n",
    "        self.initial_state = initial_state\n",
    "        self.final_states = final_states\n",
    "\n",
    "    def add_states(self, new_states):\n",
    "        self.states = self.states.union(new_states)\n",
    "        for state in new_states:\n",
    "            self.map[state] = {}\n",
    "\n",
    "    def add_transition(self, state, symbol, new_state):\n",
    "        if state not in self.map:\n",
    "            self.map[state] = {}\n",
    "        if symbol not in self.map[state]:\n",
    "            self.map[state][symbol] = {new_state}\n",
    "        else:\n",
    "            self.map[state][symbol].add(new_state)\n",
    "\n",
    "    def add_final_states(self, new_final_states):\n",
    "        self.final_states = self.final_states.union(new_final_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Er_j4Yg__b8Z"
   },
   "outputs": [],
   "source": [
    "# We create a character-level NFA to identify potential sequences of terminals.\n",
    "# For this, we assemble the DFA corresponding to the regex of each terminals.\n",
    "\n",
    "character_nfa = CharacterNFA(\n",
    "    {0},\n",
    "    characters,\n",
    "    {0: {}},\n",
    "    0,\n",
    "    set()\n",
    ")\n",
    "\n",
    "state2terminal = {0: \"\"}\n",
    "terminal2initial_state = {}\n",
    "terminal2final_states = defaultdict(set)\n",
    "\n",
    "def get_fsm(terminal):\n",
    "    return interegular.parse_pattern(terminal2regex[terminal]).to_fsm().reduce()\n",
    "\n",
    "for t in terminal2regex:\n",
    "    # We create the DFA associated with the regex of the terminal.\n",
    "    fsm = get_fsm(t)\n",
    "\n",
    "    # We map the indices used in the FSM to the corresponding characters.\n",
    "    symbol2character = defaultdict(set)\n",
    "    characters_in_map = set()\n",
    "    for k in fsm.alphabet:\n",
    "        symbol2character[fsm.alphabet[k]].add(k)\n",
    "        if type(k) == str:\n",
    "            characters_in_map.add(k)\n",
    "\n",
    "    # We add one state to the NFA for each state of the DFA.\n",
    "    delta = len(character_nfa.states)\n",
    "    new_states = {k + delta for k in fsm.states}\n",
    "    for s in new_states:\n",
    "        state2terminal[s] = t\n",
    "    character_nfa.add_states(new_states)\n",
    "    terminal2initial_state[t] = fsm.initial + delta\n",
    "    for k in fsm.finals:\n",
    "        terminal2final_states[t].add(k + delta)\n",
    "\n",
    "    # We add the edges corresponding to the edges of the DFA.\n",
    "    for node in fsm.map:\n",
    "        for symbol in fsm.map[node]:\n",
    "            origin = node + delta\n",
    "            destination = fsm.map[node][symbol] + delta\n",
    "            for character in symbol2character[symbol]:\n",
    "                if type(character) == str:\n",
    "                    if character not in characters:\n",
    "                        continue\n",
    "                    character_nfa.add_transition(origin, character, destination)\n",
    "                else:\n",
    "                    for c in characters:\n",
    "                        if c not in characters_in_map:\n",
    "                            character_nfa.add_transition(origin, c, destination)\n",
    "\n",
    "for t in terminal2final_states:\n",
    "    character_nfa.add_final_states(terminal2final_states[t])\n",
    "\n",
    "new_transitions = []\n",
    "\n",
    "# We connect the start state of the NFA with the states of the sub-DFAs.\n",
    "for t in terminal2regex:\n",
    "    initial_state = terminal2initial_state[t]\n",
    "    if initial_state in character_nfa.map:\n",
    "        for c in character_nfa.map[initial_state]:\n",
    "            for destination in character_nfa.map[initial_state][c]:\n",
    "                new_transitions.append((0, c, destination))\n",
    "\n",
    "for new_transition in new_transitions:\n",
    "    character_nfa.add_transition(*new_transition)\n",
    "del new_transitions\n",
    "\n",
    "print(len(character_nfa.states), \"states,\", sum([len(character_nfa.map[s]) for s in character_nfa.map]), \"transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "oIv54YdQOcmP"
   },
   "outputs": [],
   "source": [
    "class PartialPythonIndenter(PythonIndenter):\n",
    "    \"\"\"\n",
    "    A subclass of the Lark Python indenter to process partial strings.\n",
    "    \"\"\"\n",
    "    def handle_newline(self, terminal):\n",
    "\n",
    "        if self.paren_level > 0:\n",
    "            return []\n",
    "\n",
    "        result = [terminal]\n",
    "\n",
    "        indent_str = terminal.rsplit('\\n', 1)[1] # Tabs and spaces\n",
    "        indent = indent_str.count(' ') + indent_str.count('\\t') * self.tab_len\n",
    "\n",
    "        if indent > self.indent_level[-1]:\n",
    "            self.indent_level.append(indent)\n",
    "            result.append(lark.Token.new_borrow_pos(self.INDENT_type, indent_str, terminal))\n",
    "        else:\n",
    "            while indent < self.indent_level[-1]:\n",
    "                self.indent_level.pop()\n",
    "                result.append(lark.Token.new_borrow_pos(self.DEDENT_type, indent_str, terminal))\n",
    "\n",
    "            if indent != self.indent_level[-1]:\n",
    "                raise lark.DedentError('Unexpected dedent to column %s. Expected dedent to %s' % (indent, self.indent_level[-1]))\n",
    "        return result\n",
    "\n",
    "    def consume(self, terminals, final=False):\n",
    "        result = []\n",
    "        for terminal in terminals:\n",
    "            if terminal.type == self.NL_type:\n",
    "                result += self.handle_newline(terminal)\n",
    "            else:\n",
    "                result.append(terminal)\n",
    "\n",
    "            if terminal.type in self.OPEN_PAREN_types:\n",
    "                self.paren_level += 1\n",
    "            elif terminal.type in self.CLOSE_PAREN_types:\n",
    "                self.paren_level -= 1\n",
    "                assert self.paren_level >= 0\n",
    "\n",
    "        if final:\n",
    "            while len(self.indent_level) > 1:\n",
    "                self.indent_level.pop()\n",
    "                result.append(lark.Token(self.DEDENT_type, ''))\n",
    "            assert self.indent_level == [0], self.indent_level\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqmQ0xFrPUbO",
    "outputId": "d381d52a-97e5-436f-fc43-43bd4983798e"
   },
   "outputs": [],
   "source": [
    "# We check the syntactic validity of a Python script with the character NFA and the interactive parser.\n",
    "\n",
    "prompt = \"\"\"def fun(n):\n",
    "    if n > 0:\n",
    "        if n % 2 == 0.1: # Blabla\n",
    "            return True # Blalbla\n",
    "    # Blabla\n",
    "    return False\n",
    "\"\"\"\n",
    "\n",
    "terminals_to_ignore = set(parser.ignore_tokens)\n",
    "interactive_parser = parser.parse_interactive(\"\")\n",
    "acceptable_terminals = interactive_parser.accepts()\n",
    "indenter = PartialPythonIndenter()\n",
    "partial_string = \"\"\n",
    "path = []\n",
    "\n",
    "states = [0]\n",
    "\n",
    "def next_states(state, c):\n",
    "    if c in character_nfa.map[state]:\n",
    "        if len(character_nfa.map[state][c]) > 0:\n",
    "            return character_nfa.map[state][c], False\n",
    "    elif state in character_nfa.final_states and c in character_nfa.map[0]:\n",
    "        return character_nfa.map[0][c], True\n",
    "    return [], False\n",
    "\n",
    "for c in prompt:\n",
    "    with_new_terminals, without_new_terminals = [], []\n",
    "    highest_priority = -1\n",
    "    for state in states:\n",
    "        new_states, new_terminal = next_states(state, c)\n",
    "        for new_state in new_states:\n",
    "            if not new_terminal:\n",
    "                without_new_terminals.append(new_state)\n",
    "            else:\n",
    "                priority = terminal2priority[state2terminal[state]]\n",
    "                if priority < highest_priority:\n",
    "                    continue\n",
    "                if priority > highest_priority:\n",
    "                    highest_priority = priority\n",
    "                    with_new_terminals = []\n",
    "                with_new_terminals.append((state, new_state))\n",
    "    if len(without_new_terminals) > 0:\n",
    "        states = without_new_terminals\n",
    "        partial_string += c\n",
    "    else:\n",
    "        incoming_terminals, states = set(), []\n",
    "        for (incoming_state, new_state) in with_new_terminals:\n",
    "            incoming_terminal = state2terminal[incoming_state]\n",
    "            incoming_terminals.add(incoming_terminal)\n",
    "            states.append(new_state)\n",
    "        assert len(incoming_terminals) == 1\n",
    "        if incoming_terminal != \"\" and incoming_terminal not in terminals_to_ignore:\n",
    "            for x in indenter.consume([lark.Token(incoming_terminal, partial_string)]):\n",
    "                path.append(x)\n",
    "                interactive_parser.feed_token(x)\n",
    "        partial_string = c\n",
    "\n",
    "if len(states) > 0 and state2terminal[states[0]] == \"_NEWLINE\":\n",
    "    for x in indenter.consume([lark.Token(\"_NEWLINE\", partial_string)], final=True):\n",
    "        path.append(x)\n",
    "        interactive_parser.feed_token(x)\n",
    "\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6EjxvFdoLHJ"
   },
   "source": [
    "# Token-level NFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Rk5WOI8o29Q_"
   },
   "outputs": [],
   "source": [
    "class TokenNFA():\n",
    "    \"\"\"\n",
    "    Class to define an NFA at the token level.\n",
    "    \"\"\"\n",
    "    def __init__(self, states, initial_state, final_states):\n",
    "        self.states = states\n",
    "        self.map = {s: {} for s in states}\n",
    "        self.initial_state = initial_state\n",
    "        self.final_states = final_states\n",
    "\n",
    "    def add_transition(self, state, token_id, new_state, path, strings):\n",
    "        # We label the transitions with the corresponding sequence of terminals and associated strings.\n",
    "        if token_id not in self.map[state]:\n",
    "            self.map[state][token_id] = [(new_state, path, strings)]\n",
    "        else:\n",
    "            self.map[state][token_id].append((new_state, path, strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "kEqN04chnQYc",
    "outputId": "f725d37c-a5e2-4eef-d476-cd9de929837d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# We create the token-level NFA corresponding to the terminals of the grammar.\n",
    "# For this, we successively apply the transitions of the character-level NFA.\n",
    "# A challenge is to properly take into account the priority of the terminals.\n",
    "\n",
    "name_pattern = re.compile(terminal2regex[\"NAME\"])\n",
    "keywords = [t for t in terminal2regex if name_pattern.match(terminal2regex[t])]\n",
    "\n",
    "def get_priority(terminals, strings):\n",
    "    \"\"\"\n",
    "    Returns the priority of a path.\n",
    "    \"\"\"\n",
    "    priority, idx = [], 0\n",
    "    for i in range(len(terminals)):\n",
    "        idx += len(strings[i])\n",
    "        priority.append(idx)\n",
    "        priority.append(terminal2priority[terminals[i]])\n",
    "    return priority\n",
    "\n",
    "def compare(p1, p2):\n",
    "    \"\"\"\n",
    "    Compare two priorities (represented as a tuple).\n",
    "    \"\"\"\n",
    "    length = min(len(p1), len(p2))\n",
    "    p1, p2 = p1[:length], p2[:length]\n",
    "    if p1 < p2:\n",
    "        return -1\n",
    "    return 0 if p1 == p2 else 1\n",
    "\n",
    "token_nfa = TokenNFA(character_nfa.states, character_nfa.initial_state, character_nfa.final_states)\n",
    "for state in token_nfa.states:\n",
    "    start_terminal = state2terminal[state]\n",
    "    # The `traverse_callback` function is later used to traverse the trie to avoid redundant computations.\n",
    "    def traverse_callback(path_conv, path, children, token_id=-1):\n",
    "        # We check if the current node corresponds to a token (with token_id ≥ 0)\n",
    "        if token_id >= 0:\n",
    "            token = path_conv(path)\n",
    "            if len(token) == 1:\n",
    "                # If the length of the token is 1, we simply copy the transition of the character NFA\n",
    "                if token in character_nfa.map[state]:\n",
    "                    for new_state in character_nfa.map[state][token]:\n",
    "                        if state == 0:\n",
    "                            terminals = [\"\", state2terminal[new_state]]\n",
    "                            strings = [\"\", token]\n",
    "                        else:\n",
    "                            terminals = [start_terminal]\n",
    "                            strings = [token]\n",
    "                        token_nfa.add_transition(state, token_id, new_state, terminals, strings)\n",
    "                elif state in character_nfa.final_states and state != 0 and token in character_nfa.map[0]:\n",
    "                    if state2terminal[state] not in keywords or not name_pattern.match(token):\n",
    "                        for new_state in character_nfa.map[0][token]:\n",
    "                            terminals = [start_terminal, state2terminal[new_state]]\n",
    "                            strings = [\"\", token]\n",
    "                            token_nfa.add_transition(state, token_id, new_state, terminals, strings)\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                # If the length is > 1, we start from the longest strict prefix of the token...\n",
    "                previous_token, previous_token_id = trie.longest_prefix(token[:-1])\n",
    "                if previous_token_id in token_nfa.map[state]:\n",
    "                    stack = token_nfa.map[state][previous_token_id]\n",
    "                else:\n",
    "                    return 0\n",
    "                #... and for each character between the prefix and the token, we follow the character NFA.\n",
    "                for i in range(len(previous_token), len(token)):\n",
    "                    c = token[i]\n",
    "                    new_configurations = []\n",
    "                    for current_state, current_terminals, strings in stack:\n",
    "                        if c in character_nfa.map[current_state]:\n",
    "                            for new_state in character_nfa.map[current_state][c]:\n",
    "                                if current_state == 0:\n",
    "                                    # If we are at the start state, the character leads to a new state.\n",
    "                                    new_configurations.append((new_state, [state2terminal[new_state]], [c]))\n",
    "                                else:\n",
    "                                    # Otherwise, we necessarily stay in the same terminal DFA.\n",
    "                                    new_configurations.append((new_state, current_terminals, strings[:-1] + [strings[-1] + c]))\n",
    "                        elif current_state in character_nfa.final_states and current_state != 0 and c in character_nfa.map[0]:\n",
    "                            if state2terminal[current_state] in keywords:\n",
    "                                if name_pattern.match(c):\n",
    "                                    continue\n",
    "                            for new_state in character_nfa.map[0][c]:\n",
    "                                new_configurations.append((new_state, current_terminals + [state2terminal[new_state]], strings + [c]))\n",
    "                    # We favor continuations which don't lead to a new terminal\n",
    "                    stack = []\n",
    "                    if len(new_configurations) == 0:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        prioritized = defaultdict(list)\n",
    "                        for k in new_configurations:\n",
    "                            new_priority = tuple(get_priority(*k[1:]))\n",
    "                            if len(prioritized) == 0:\n",
    "                                prioritized[tuple(new_priority)].append(k)\n",
    "                            else:\n",
    "                                to_add, to_remove = False, set()\n",
    "                                for priority in list(prioritized.keys()):\n",
    "                                    comparison = compare(new_priority[:-2], priority[:-2])\n",
    "                                    if comparison == 0:\n",
    "                                        to_add = True\n",
    "                                    elif comparison == 1:\n",
    "                                        to_add = True\n",
    "                                        to_remove.add(priority)\n",
    "                                    else:\n",
    "                                        break\n",
    "                                else:\n",
    "                                    if to_add:\n",
    "                                        prioritized[tuple(new_priority)].append(k)\n",
    "                                        for priority in to_remove:\n",
    "                                            del prioritized[priority]\n",
    "                        for p in prioritized:\n",
    "                            stack += prioritized[p]\n",
    "\n",
    "                for (new_state, terminals, strings) in stack:\n",
    "                    token_nfa.add_transition(state, token_id, new_state, terminals, strings)\n",
    "\n",
    "        return sum(children)\n",
    "\n",
    "    trie.traverse(traverse_callback)\n",
    "\n",
    "num_transitions = 0\n",
    "for s in token_nfa.map:\n",
    "    for token_id in token_nfa.map[s]:\n",
    "        num_transitions += len(token_nfa.map[s][token_id])\n",
    "print(len(token_nfa.states), \"states,\", num_transitions, \"transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zXLyPtb6a7VA"
   },
   "outputs": [],
   "source": [
    "# In `token_nfa.next_tokens` and `token_nfa.mask`, we rearrange the transition function\n",
    "# to group together the tokens that lead to the same sequence of terminals\n",
    "\n",
    "# The signature of `token_nfa.next_tokens` is state x (sequence_of_terminals, new_terminal) --> set of tokens\n",
    "# where `new_terminal` is a boolean equal to True if the first characters remain in the current terminal.\n",
    "\n",
    "# The signature of `token_nfa.mask` is state x (sequence_of_terminals, new_terminal) --> mask\n",
    "# where `mask` is a binary vector whose length is the size of the vocabulary.\n",
    "\n",
    "token_nfa.next_tokens = {}\n",
    "for s in token_nfa.map:\n",
    "    terminal = state2terminal[s]\n",
    "    token_nfa.next_tokens[s] = {}\n",
    "    for token_id in token_nfa.map[s]:\n",
    "        for (_, new_terminals, strings) in token_nfa.map[s][token_id]:\n",
    "            path = tuple([t for t in new_terminals[1:] if t not in parser.ignore_tokens])\n",
    "            new_characters = len(strings[0]) > 0\n",
    "            if (path, new_characters) not in token_nfa.next_tokens[s]:\n",
    "                token_nfa.next_tokens[s][(path, new_characters)] = set([token_id])\n",
    "            else:\n",
    "                token_nfa.next_tokens[s][(path, new_characters)].add(token_id)\n",
    "\n",
    "token_nfa.mask = {}\n",
    "to_delete = set()\n",
    "for s in token_nfa.next_tokens:\n",
    "    token_nfa.mask[s] = {}\n",
    "    for (path, new_characters) in token_nfa.next_tokens[s]:\n",
    "        if len(token_nfa.next_tokens[s][(path, new_characters)]) > 0:\n",
    "            mask = torch.zeros((tokenizer.vocab_size,), dtype=torch.bool)\n",
    "            mask[list(token_nfa.next_tokens[s][(path, new_characters)])] = 1\n",
    "            token_nfa.mask[s][(path, new_characters)] = mask\n",
    "        else:\n",
    "            to_delete.add((s, k))\n",
    "for s, k in to_delete:\n",
    "    del token_nfa.next_tokens[s][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRP32X9_UGuH",
    "outputId": "4d85cc91-26b6-46ed-c24a-a5a443490b98"
   },
   "outputs": [],
   "source": [
    "print(\"Number of entries in the mask store:\", sum(len(token_nfa.mask[k]) for k in token_nfa.mask))\n",
    "\n",
    "unique_paths = set()\n",
    "for s in token_nfa.next_tokens:\n",
    "    for k in token_nfa.next_tokens[s]:\n",
    "        unique_paths.add((state2terminal[s],) + k)\n",
    "print(len(unique_paths), \"unique configurations that can be tested at inference time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ZENeZ421uyGW"
   },
   "outputs": [],
   "source": [
    "def get_length_common_prefix(strings):\n",
    "    prefix = None\n",
    "    for s in strings:\n",
    "        if prefix is None:\n",
    "            prefix = s[:-1]\n",
    "        else:\n",
    "            length = min(len(prefix), len(s)-1)\n",
    "            for i in range(length):\n",
    "                if prefix[i] != s[i]:\n",
    "                    prefix = prefix[:i]\n",
    "                    break\n",
    "            else:\n",
    "                prefix = prefix[:length]\n",
    "\n",
    "    return len(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "BPI5FAGeC06z"
   },
   "outputs": [],
   "source": [
    "mask_leading_space = torch.zeros((tokenizer.vocab_size,), dtype=torch.bool)\n",
    "mask_no_leading_space = torch.zeros((tokenizer.vocab_size,), dtype=torch.bool)\n",
    "mask_leading_space[[vocabulary[w] for w in vocabulary if w.startswith(\" \") and not w.startswith(\"  \")]] = 1\n",
    "mask_no_leading_space[[vocabulary[w] for w in vocabulary if not w.startswith(\" \")]] = 1\n",
    "linebreak_token_id = tokenizer.encode(\"\\n\")[-1]\n",
    "\n",
    "class PythonMaskGenerator():\n",
    "    \"\"\"\n",
    "    Class to generate masks to enforce a valid Python syntax.\n",
    "    \"\"\"\n",
    "    def __init__(self, parser, token_nfa):\n",
    "        self.token_nfa = token_nfa\n",
    "        self.parser = parser\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the states of the mask generator to generate a new string.\n",
    "        \"\"\"\n",
    "        self.indenter = PartialPythonIndenter()\n",
    "        self.states = [(0, [\"\"], [\"\"], -1)]\n",
    "        self.partial_path = []\n",
    "        self.partial_strings = []\n",
    "        self.interactive_parser = self.parser.parse_interactive(\"\")\n",
    "        self.interactive_parser2 = self.parser.parse_interactive(\"\")\n",
    "        self.acceptable_terminals = self.interactive_parser.accepts()\n",
    "        self.temp_interactive_parser = None\n",
    "\n",
    "    def consume(self, token_id):\n",
    "        \"\"\"\n",
    "        Update the states of the mask generator by consuming one token.\n",
    "        \"\"\"\n",
    "        new_states = []\n",
    "        prioritized = defaultdict(list)\n",
    "        for (state, path, strings, _) in self.states:\n",
    "            if token_id in self.token_nfa.map[state]:\n",
    "                for (new_state, new_terminals, new_strings) in self.token_nfa.map[state][token_id]:\n",
    "                    new_path = path + new_terminals[1:]\n",
    "                    new_strings = strings[:-1] + [strings[-1] + new_strings[0]] + new_strings[1:]\n",
    "                    new_priority = tuple(get_priority(new_path, new_strings))\n",
    "                    if len(prioritized) == 0:\n",
    "                        prioritized[new_priority].append((new_state, new_path, new_strings, new_priority))\n",
    "                    else:\n",
    "                        to_add, to_remove = False, set()\n",
    "                        for priority in list(prioritized.keys()):\n",
    "                            comparison = compare(new_priority[:-2], priority[:-2])\n",
    "                            if comparison == 0:\n",
    "                                to_add = True\n",
    "                            elif comparison == 1:\n",
    "                                to_add = True\n",
    "                                to_remove.add(priority)\n",
    "                            else:\n",
    "                                break\n",
    "                        else:\n",
    "                            if to_add:\n",
    "                                prioritized[new_priority].append((new_state, new_path, new_strings, new_priority))\n",
    "                                for priority in to_remove:\n",
    "                                    del prioritized[priority]\n",
    "\n",
    "        for p in prioritized:\n",
    "            new_states += prioritized[p]\n",
    "            \n",
    "        length_common_prefix = get_length_common_prefix([s[1] for s in new_states])\n",
    "        (x, p, s, _) = new_states[0]\n",
    "        for i in range(length_common_prefix):\n",
    "            if (p[i] not in terminals_to_ignore and p[i] != \"\"):\n",
    "                for x in self.indenter.consume([lark.Token(p[i], s[i])]):\n",
    "                    self.partial_path.append(x)\n",
    "                    self.interactive_parser.feed_token(x)\n",
    "                    self.acceptable_terminals = self.interactive_parser.accepts()\n",
    "\n",
    "        if self.indenter.paren_level > 0:\n",
    "            new_states = [\n",
    "                (\n",
    "                    20 if (s[-1][-1] == \"\\n\" and p[-1] in [\"_NEWLINE\", \"__IGNORE_0\"]) else x,\n",
    "                    [\"__IGNORE_0\" if t == \"_NEWLINE\" else t for t in p],\n",
    "                    s,\n",
    "                    priority\n",
    "                )\n",
    "                for (x, p, s, priority) in new_states\n",
    "        ]\n",
    "        self.states = sorted(\n",
    "            [(x, p[length_common_prefix:], s[-len(p)+length_common_prefix:], priority) for (x, p, s, priority) in new_states],\n",
    "            key=lambda t: t[-1],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "    def terminate(self):\n",
    "        \"\"\"\n",
    "        Update the states of the mask generator by terminating the current string.\n",
    "        \"\"\"\n",
    "        if len(self.states) > 0:\n",
    "            if len(self.states[0][1]) > 0 and self.states[0][1][0] == \"_NEWLINE\":\n",
    "                for x in self.indenter.consume([lark.Token(\"_NEWLINE\", self.states[0][2][0])], final=True):\n",
    "                    self.partial_path.append(x)\n",
    "                    self.interactive_parser.feed_token(x)\n",
    "\n",
    "    def validate_terminals(self, terminals, parentheses):\n",
    "        \"\"\"\n",
    "        Check whether a sequence of terminals is acceptable given the current states of the mask generator.\n",
    "        \"\"\"\n",
    "        if parentheses:\n",
    "            terminals = [t for t in terminals if t not in terminals_to_ignore and t != \"\" and t != \"_NEWLINE\"]\n",
    "        else:\n",
    "            terminals = [t for t in terminals if t not in terminals_to_ignore and t != \"\"]\n",
    "        if len(terminals) == 0:\n",
    "            return True\n",
    "        if terminals[0] not in self.acceptable_terminals:\n",
    "            return False\n",
    "        if len(terminals) == 1:\n",
    "            return True\n",
    "        self.interactive_parser2.parser_state = self.interactive_parser.parser_state.copy()\n",
    "        try:\n",
    "            for t in terminals:\n",
    "                self.interactive_parser2.feed_token(lark.Token(t, \"\"))\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def get_mask(self, indent=False, streamlined=False, parentheses=False):\n",
    "        \"\"\"\n",
    "        Return the mask corresponding to the current states (when the last token was not a new line).\n",
    "        \"\"\"\n",
    "        mask = torch.zeros((tokenizer.vocab_size,), dtype=torch.bool)\n",
    "        final_state_already_seen = False\n",
    "        for (state, terminals, _, _) in self.states:\n",
    "            current_path = tuple([t for t in terminals if t not in terminals_to_ignore])\n",
    "            terminal = state2terminal[state]\n",
    "            if len(terminal) > 0 and terminal not in terminals_to_ignore:\n",
    "                if terminal not in self.acceptable_terminals:\n",
    "                    if not parentheses or terminal != \"_NEWLINE\":\n",
    "                        if state in self.token_nfa.final_states:\n",
    "                            final_state_already_seen = True\n",
    "                        continue\n",
    "            if streamlined:\n",
    "                for (path, new_characters) in self.token_nfa.streamlined_mask[state]:\n",
    "                    #if self.states[0][2][-1] == 'class':\n",
    "                    #    print(\"ççç\", state, terminal, k)\n",
    "                    if final_state_already_seen:\n",
    "                        if not new_characters:\n",
    "                            continue\n",
    "                    to_check = current_path + ((\"_INDENT\",) if indent else ()) + path\n",
    "                    if self.validate_terminals(to_check, parentheses):\n",
    "                        mask = torch.bitwise_or(self.token_nfa.streamlined_mask[state][(path, new_characters)], mask)\n",
    "                        if parentheses and ((path == () and not new_characters) or terminal in terminals_to_ignore) and state in self.token_nfa.final_states:\n",
    "                            mask[linebreak_token_id] = True\n",
    "                if state in self.token_nfa.final_states:\n",
    "                    final_state_already_seen = True\n",
    "            else:\n",
    "                for (path, new_characters) in self.token_nfa.mask[state]:\n",
    "                    #if self.states[0][2][-1] == 'class':\n",
    "                    #    print(\"ççç\", state, terminal, k)\n",
    "                    if final_state_already_seen:\n",
    "                        if not new_characters:\n",
    "                            continue\n",
    "                    to_check = current_path + ((\"_INDENT\",) if indent else ()) + path\n",
    "                    if self.validate_terminals(to_check, parentheses):\n",
    "                        mask = torch.bitwise_or(self.token_nfa.mask[state][(path, new_characters)], mask)\n",
    "                        if parentheses and ((path == () and not new_characters) or terminal in terminals_to_ignore) and state in self.token_nfa.final_states:\n",
    "                            mask[linebreak_token_id] = True\n",
    "                if state in self.token_nfa.final_states:\n",
    "                    final_state_already_seen = True\n",
    "        return mask\n",
    "\n",
    "    def build_mask(self, streamlined=False):\n",
    "        \"\"\"\n",
    "        Return the mask corresponding to the current states.\n",
    "        \"\"\"\n",
    "        state = self.states[0]\n",
    "        if len(state[1]) == 0:\n",
    "            mask = self.get_mask(streamlined=streamlined)\n",
    "            mask[linebreak_token_id] = True\n",
    "            return mask\n",
    "        if len(state[1]) > 0 and state[1][-1] == \"_NEWLINE\":\n",
    "            if \"\\n\" in state[2][-1] and self.indenter.paren_level == 0:\n",
    "                line_content = state[2][-1].split(\"\\n\")[-1]\n",
    "                if len([c for c in line_content if c != \" \"]) == 0:\n",
    "                    num_spaces = len(line_content)\n",
    "                    parser_state = self.interactive_parser.parser_state.copy()\n",
    "                    self.interactive_parser2.parser_state = parser_state\n",
    "                    self.interactive_parser2.feed_token(lark.Token(\"_NEWLINE\", state[2][-1]))\n",
    "                    if \"_INDENT\" in self.interactive_parser2.accepts():\n",
    "                        target_indentation = self.indenter.indent_level[-1] + 4\n",
    "                        delta_indentation = target_indentation - num_spaces - 1\n",
    "                        if delta_indentation == 0:\n",
    "                            mask = torch.bitwise_and(self.get_mask(indent=True), mask_leading_space)\n",
    "                        else:\n",
    "                            mask = torch.zeros((tokenizer.vocab_size,), dtype=torch.bool)\n",
    "                            mask[tokenizer.encode(\" \"*delta_indentation)[1]] = 1\n",
    "                    else:\n",
    "                        if 0 in self.indenter.indent_level and num_spaces == 0:\n",
    "                            mask = torch.bitwise_and(\n",
    "                                self.get_mask(),\n",
    "                                mask_no_leading_space\n",
    "                            )\n",
    "                        else:\n",
    "                            mask = torch.zeros((tokenizer.vocab_size,), dtype=torch.bool)\n",
    "\n",
    "                        for indent_level in self.indenter.indent_level[1:]:\n",
    "                            delta_indentation = indent_level - num_spaces - 1\n",
    "                            if delta_indentation == 0:\n",
    "                                mask = torch.bitwise_or(\n",
    "                                    mask,\n",
    "                                    torch.bitwise_and(\n",
    "                                        self.get_mask(streamlined=streamlined),\n",
    "                                        mask_leading_space\n",
    "                                    )\n",
    "                                )\n",
    "                            elif delta_indentation > 0:\n",
    "                                mask[tokenizer.encode(\" \"*delta_indentation)[1]] = 1\n",
    "                    mask[linebreak_token_id] = True\n",
    "                    return mask\n",
    "        if self.indenter.paren_level > 0 or (len(self.states[0][1]) > 0 and self.states[0][1][-1] in [\"LPAR\", \"LBRACE\", \"LSQB\"]):\n",
    "            mask = self.get_mask(streamlined=streamlined, parentheses=True)\n",
    "            return mask\n",
    "\n",
    "        return self.get_mask(streamlined=streamlined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "njijVAE2U56J",
    "outputId": "f22204dd-96e5-4be2-a3e8-891c72441e5e"
   },
   "outputs": [],
   "source": [
    "# Test the mask generator with a prompt\n",
    "\n",
    "token_ids = tokenizer(prompt)[\"input_ids\"][1:]\n",
    "\n",
    "mask_generator = PythonMaskGenerator(parser, token_nfa)\n",
    "mask = mask_generator.build_mask(streamlined=False)\n",
    "for token_id in token_ids:\n",
    "    # Check that the mask accepts the next token\n",
    "    assert mask[token_id]\n",
    "    mask_generator.consume(token_id)\n",
    "    mask = mask_generator.build_mask(streamlined=False)\n",
    "mask_generator.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyN7cECdFga7"
   },
   "source": [
    "# `is_never_legal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "C2IOHXsxeCCI"
   },
   "outputs": [],
   "source": [
    "variables = set()\n",
    "terminals = set()\n",
    "start_symbol = Variable(parser.parser.parser_conf.start[0])\n",
    "productions = set()\n",
    "\n",
    "# Build the CFG with only one element of each set of interchangeable terminals\n",
    "rules = []\n",
    "for rule in parser.rules:\n",
    "    name = get_name(rule.origin)\n",
    "    head = Variable(name)\n",
    "    body, body2 = [], []\n",
    "    variables.add(head)\n",
    "    for x in rule.expansion:\n",
    "        if type(x) == lark.grammar.NonTerminal:\n",
    "            variable = Variable(x.name)\n",
    "            variables.add(variable)\n",
    "            body.append(variable)\n",
    "        elif type(x) == lark.grammar.Terminal:\n",
    "            if x.name in parser.ignore_tokens or x.name in terminals_to_replace:\n",
    "                break\n",
    "            terminal = Terminal(x.name)\n",
    "            terminals.add(terminal)\n",
    "            body.append(terminal)\n",
    "        body2.append(x.name)\n",
    "    else:\n",
    "        productions.add(Production(head, body))\n",
    "        rules.append((name, body2))\n",
    "\n",
    "cfg = CFG(variables=variables, terminals=terminals, productions=productions, start_symbol=start_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "Txzh0k_1dUOq"
   },
   "outputs": [],
   "source": [
    "def is_never_legal(terminal, target_sequence):\n",
    "    \"\"\"\n",
    "    Return True if the terminals in `target_sequence` can never follow `terminal`.\n",
    "    \"\"\"\n",
    "    if len(target_sequence) == 0:\n",
    "        return True\n",
    "    if terminal == \"\":\n",
    "        anything = f\"(__ANYTHING_ELSE__|{'|'.join(target_sequence)})*\"\n",
    "        regex_str = f\"{'.'.join(target_sequence)}.{anything}\"\n",
    "    elif terminal in parser.lexer_conf.ignore:\n",
    "        anything = f\"(__ANYTHING_ELSE__|{'|'.join(target_sequence)})*\"\n",
    "        regex_str = f\"{anything}.{'.'.join(target_sequence)}.{anything}\"\n",
    "    else:\n",
    "        anything = f\"(__ANYTHING_ELSE__|{'|'.join(set([terminal] + target_sequence))})*\"\n",
    "        regex_str = f\"{anything}.{terminal}.{'.'.join(target_sequence)}.{anything}\"\n",
    "    regex = Regex(regex_str)\n",
    "\n",
    "    dfa = regex.to_epsilon_nfa().minimize()\n",
    "    all_terminals = [t.value for t in terminals if t.value not in terminals_to_ignore and t.value not in terminals_to_replace]\n",
    "    transitions = dfa._transition_function.to_dict()\n",
    "    for state in transitions:\n",
    "        if \"__ANYTHING_ELSE__\" in transitions[state]:\n",
    "            target = transitions[state][\"__ANYTHING_ELSE__\"]\n",
    "            for terminal in all_terminals:\n",
    "                if terminal not in transitions[state]:\n",
    "                    dfa.add_transition(state, terminal, target)\n",
    "            dfa.remove_transition(state, \"__ANYTHING_ELSE__\", target)\n",
    "    return cfg.intersection(dfa).is_empty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_S0qq-vIDa3"
   },
   "source": [
    "# `is_always_legal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FyZKoKNEPCOh"
   },
   "outputs": [],
   "source": [
    "variables = set()\n",
    "transitions = defaultdict(set)\n",
    "# transitions[state] = (symbol_to_read, stack_symbol_to_pop, new_state, stack_symbol_to_add)\n",
    "\n",
    "# Build the transition relation of the DFT pushdown automaton.\n",
    "for rule in cfg.productions:\n",
    "    variables.add(rule.head.value)\n",
    "    if len(rule.body) == 0:\n",
    "        transitions[(rule.head.value, \"variable_start\")].add((\"@epsilon\", \"@epsilon\", (rule.head.value, \"variable_end\"), \"@epsilon\"))\n",
    "        continue\n",
    "\n",
    "    current = (rule.head.value, \"variable_start\"), \"@epsilon\"\n",
    "    for i in range(len(rule.body)):\n",
    "        symbol = rule.body[i]\n",
    "        location = rule.head.value + \":\" + \".\".join([rule.body[i].value for i in range(i+1)])\n",
    "        if type(symbol) == Variable:\n",
    "            transitions[current[0]].add((\"@epsilon\", current[1], (symbol.value, \"variable_start\"), location))\n",
    "            current = (symbol.value, \"variable_end\"), location\n",
    "        else:\n",
    "            transitions[current[0]].add((symbol.value, current[1], (symbol.value, \"terminal\"), location))\n",
    "            current = (symbol.value, \"terminal\"), location\n",
    "    transitions[current[0]].add((\"@epsilon\", current[1], (rule.head.value, \"variable_end\"), \"@epsilon\"))\n",
    "\n",
    "next_potential_terminals = defaultdict(set)\n",
    "\n",
    "# Use the DFT pushdown automaton to list terminals that may follow a given symbol.\n",
    "for symbol in [(t.value, \"terminal\") for t in terminals] + [(v, \"variable_start\") for v in variables] + [(v, \"variable_end\") for v in variables]:\n",
    "    queue = [symbol]\n",
    "    already_seen = set()\n",
    "    while len(queue) > 0:\n",
    "        current = queue.pop()\n",
    "        if current in transitions:\n",
    "            for (to_read, to_pop, destination, to_add) in transitions[current]:\n",
    "                if destination[1] == \"terminal\":\n",
    "                    next_potential_terminals[symbol].add(destination[0])\n",
    "                elif destination not in already_seen:\n",
    "                    already_seen.add(destination)\n",
    "                    queue.append(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "4M1Fef7bTIS1"
   },
   "outputs": [],
   "source": [
    "class InstantaneousDescription:\n",
    "    \"\"\"\n",
    "    Class to describe each step of the search trajectories within the DFT pushdown automaton.\n",
    "    \"\"\"\n",
    "    def __init__(self, state, sequence):\n",
    "        self.state = state\n",
    "        self.stack = ()\n",
    "        self.sequence = sequence\n",
    "        self.idx = 0\n",
    "        self.history = ((state, 0),)\n",
    "\n",
    "    def apply_transition(self, to_read, to_pop, destination, to_add):\n",
    "        if to_read != \"@epsilon\":\n",
    "            if self.sequence[self.idx] == to_read:\n",
    "                self.idx = self.idx + 1\n",
    "            else:\n",
    "                return None\n",
    "        if to_pop != \"@epsilon\":\n",
    "            if len(self.stack) == 0:\n",
    "                self.history = self.history + (to_pop, (destination, self.stack, self.idx))\n",
    "            elif self.stack[-1] == to_pop:\n",
    "                self.stack = self.stack[:-1]\n",
    "                self.history = self.history + (\"@epsilon\", (destination, self.stack, self.idx))\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            self.history = self.history + (\"@epsilon\", (destination, self.stack, self.idx))\n",
    "        if to_add != \"@epsilon\":\n",
    "            self.stack = self.stack + (to_add,)\n",
    "        self.state = destination\n",
    "        return self\n",
    "\n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "def find_trajectories(symbol, target_sequence, maximum_delta=20, terminal=True):\n",
    "    \"\"\"\n",
    "    Find trajectories from `symbol` going through `target_sequence`.\n",
    "    `maximum_delta` is a termination criterion based on the stack height so that the computation eventually ends.\n",
    "    \"\"\"\n",
    "    successful_paths = []\n",
    "    uncertain_paths = []\n",
    "    incomplete_paths = []\n",
    "    already_seen = defaultdict(set)\n",
    "    front = [InstantaneousDescription((symbol, \"terminal\" if terminal else \"variable_start\"), target_sequence)]\n",
    "    while len(front) > 0:\n",
    "        new_front = []\n",
    "        for current in front:\n",
    "            for (to_read, to_pop, destination, to_add) in transitions[current.state]:\n",
    "                new_current = current.copy().apply_transition(to_read, to_pop, destination, to_add)\n",
    "                if new_current is None:\n",
    "                    continue\n",
    "\n",
    "                if (new_current.state, new_current.idx) in already_seen:\n",
    "                    to_continue = False\n",
    "                    for recorded_stack in already_seen[(new_current.state, new_current.idx)]:\n",
    "                        if recorded_stack == new_current.stack:\n",
    "                            uncertain_paths.append(new_current)\n",
    "                            to_continue = True\n",
    "                            break\n",
    "                        elif len(new_current.stack) > len(recorded_stack) + maximum_delta:\n",
    "                            incomplete_paths.append(new_current)\n",
    "                            to_continue = True\n",
    "                            break\n",
    "                    if to_continue:\n",
    "                        continue\n",
    "                already_seen[(new_current.state, new_current.idx)].add(new_current.stack)\n",
    "\n",
    "                if new_current.idx == len(new_current.sequence):\n",
    "                    successful_paths.append(new_current)\n",
    "                    continue\n",
    "\n",
    "                if new_current.state[1] != \"terminal\":\n",
    "                    if new_current.sequence[new_current.idx] not in next_potential_terminals[new_current.state]:\n",
    "                        continue\n",
    "\n",
    "                new_front.append(new_current)\n",
    "\n",
    "        front = new_front\n",
    "    return successful_paths, uncertain_paths, incomplete_paths\n",
    "\n",
    "def build_nfa(successful_paths, uncertain_paths):\n",
    "    \"\"\"\n",
    "    Build the debt NFA based on the trajectories obtained through the search.\n",
    "    \"\"\"\n",
    "    nfa = EpsilonNFA()\n",
    "\n",
    "    nfa.add_start_state(successful_paths[0].history[0])\n",
    "    for successful_path in successful_paths:\n",
    "        nfa.add_final_state(successful_path.history[-1])\n",
    "\n",
    "    for path in successful_paths:\n",
    "        for i in range(0, len(path.history) - 2, 2):\n",
    "            start = path.history[i]\n",
    "            transition = path.history[i+1]\n",
    "            end = path.history[i+2]\n",
    "            transition = path.history[i+1].replace(\"@epsilon\", \"epsilon\")\n",
    "            nfa.add_transitions([(start, transition, end)])\n",
    "\n",
    "    for path in uncertain_paths:\n",
    "        if path.history[-1] in nfa.states:\n",
    "            for i in range(0, len(path.history) - 2, 2):\n",
    "                start = path.history[i]\n",
    "                transition = path.history[i+1]\n",
    "                end = path.history[i+2]\n",
    "                transition = path.history[i+1].replace(\"@epsilon\", \"epsilon\")\n",
    "                nfa.add_transitions([(start, transition, end)])\n",
    "\n",
    "    return nfa\n",
    "\n",
    "def accepts_empty_word(nfa):\n",
    "    \"\"\"\n",
    "    Check whether the initial state of the NFA is ε-coaccessible.\n",
    "    \"\"\"\n",
    "    final_states = list(nfa.final_states)[0]\n",
    "    start_state = list(nfa.start_states)[0]\n",
    "    nfa_transitions = nfa._transition_function.to_dict()\n",
    "    epsilon_coaccessible_states = set()\n",
    "    for s in nfa.states:\n",
    "        eclose = nfa.eclose(s)\n",
    "        for final_state in nfa.final_states:\n",
    "            if final_state in eclose:\n",
    "                epsilon_coaccessible_states.add(s)\n",
    "                break\n",
    "    num_new_states = len(epsilon_coaccessible_states)\n",
    "    if start_state in epsilon_coaccessible_states:\n",
    "        return True\n",
    "    while num_new_states > 0:\n",
    "        num_new_states = 0\n",
    "        for s in nfa.states:\n",
    "            if s not in epsilon_coaccessible_states:\n",
    "                if s in nfa_transitions:\n",
    "                    for k in nfa_transitions[s]:\n",
    "                        if k.value == \"epsilon\":\n",
    "                            for destination in nfa_transitions[s][k]:\n",
    "                                if destination in epsilon_coaccessible_states:\n",
    "                                    if s == start_state:\n",
    "                                        return True\n",
    "                                    epsilon_coaccessible_states.add(s)\n",
    "                                    num_new_states += 1\n",
    "                                    break\n",
    "                    else:\n",
    "                        if len(transitions[s.value[0]]) == len(nfa_transitions[s]):\n",
    "                            for k in nfa_transitions[s]:\n",
    "                                for destination in nfa_transitions[s][k]:\n",
    "                                    if destination in epsilon_coaccessible_states:\n",
    "                                        break\n",
    "                                else:\n",
    "                                    break\n",
    "                            else:\n",
    "                                if s == start_state:\n",
    "                                    return True\n",
    "                                epsilon_coaccessible_states.add(s)\n",
    "                                num_new_states += 1\n",
    "    return False\n",
    "\n",
    "@functools.lru_cache()\n",
    "def accepts(symbol, target_sequence, terminal=True, maximum_delta=20):\n",
    "    \"\"\"\n",
    "    Attempt to check whether `symbol` followed by `target_sequence` is always possible or never possible.\n",
    "    \"\"\"\n",
    "    successful_paths, uncertain_paths, incomplete_paths = find_trajectories(symbol, target_sequence, terminal=terminal, maximum_delta=maximum_delta)\n",
    "    if len(successful_paths) == 0:\n",
    "        if len(incomplete_paths) == 0:\n",
    "            return \"never legal\"\n",
    "        else:\n",
    "            if symbol == \"file_input\":\n",
    "                potentially_legal = not is_never_legal(\"\", list(target_sequence))\n",
    "            else:\n",
    "                potentially_legal = not is_never_legal(symbol, list(target_sequence))\n",
    "            if potentially_legal:\n",
    "                nfa = build_nfa(successful_paths + incomplete_paths, uncertain_paths)\n",
    "                if accepts_empty_word(nfa):\n",
    "                    return \"possibly always legal\"\n",
    "                else:\n",
    "                    return \"sometimes legal\"\n",
    "            else:\n",
    "                return \"never legal\"\n",
    "    nfa = build_nfa(successful_paths, uncertain_paths)\n",
    "    if accepts_empty_word(nfa):\n",
    "        return \"always legal\"\n",
    "    elif len(incomplete_paths) == 0:\n",
    "        return \"sometimes legal\"\n",
    "    else:\n",
    "        nfa = build_nfa(successful_paths + incomplete_paths, uncertain_paths)\n",
    "        if accepts_empty_word(nfa):\n",
    "            return \"possibly always legal\"\n",
    "        else:\n",
    "            return \"sometimes legal\"\n",
    "\n",
    "def draw(nfa):\n",
    "    G = nfa.to_networkx()\n",
    "    label_replacements = {\n",
    "        'ɛ': '&#949;',   # Replace epsilon with e\n",
    "    }\n",
    "\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if \"label\" in data:\n",
    "            if data['label'] in label_replacements:\n",
    "                data['label'] = label_replacements[data['label']]\n",
    "\n",
    "    A = to_agraph(G)\n",
    "    A.graph_attr['rankdir'] = 'LR'  # Left to right layout\n",
    "\n",
    "    with io.BytesIO() as buffer:\n",
    "        A.draw(\"/content/automaton.png\", format='png', prog='dot')\n",
    "        A.draw(buffer, format='png', prog='dot')\n",
    "        buffer.seek(0)\n",
    "        img = plt.imread(buffer)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask store streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8k-KCaLblYm",
    "outputId": "f218dcc0-7e2d-4839-b622-b665c2cfa491"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if False:#\"configuration_maps.pkl\" in os.listdir():\n",
    "    with open(\"configuration_maps.pkl\", \"rb\") as f:\n",
    "        (\n",
    "            configuration_map,\n",
    "            configuration_map2,\n",
    "            configuration_map3,\n",
    "            configuration_map4,\n",
    "            possibly_always_legal,\n",
    "        ) = pickle.load(f)\n",
    "else:\n",
    "    unique_configurations = set()\n",
    "    configuration_map = {}\n",
    "    configuration_map2 = {}\n",
    "    configuration_map3 = {}\n",
    "    configuration_map4 = {}\n",
    "    for state in token_nfa.next_tokens:\n",
    "        terminal = state2terminal[state]\n",
    "        for (target_sequence, new_characters) in token_nfa.next_tokens[state]:\n",
    "            unique_configurations.add((terminal, (target_sequence, new_characters)))\n",
    "\n",
    "    # Exploit terminal interchangeability to merge equivalent configurations\n",
    "    for (terminal, (target_sequence, new_characters)) in unique_configurations:\n",
    "        terminal2 = terminal_replacement[terminal] if terminal in terminal_replacement else terminal\n",
    "        target_sequence2 = [terminal_replacement[t] if t in terminal_replacement else t for t in target_sequence]\n",
    "        configuration_map[(terminal, (target_sequence, new_characters))] = (terminal2, (tuple(target_sequence2), new_characters))\n",
    "\n",
    "    # Use 1-hop successor relationships between terminals to remove impossible configurations\n",
    "    for (terminal, (target_sequence, new_characters)) in set(configuration_map.values()):\n",
    "        if len(target_sequence) == 0:\n",
    "            configuration_map2[(terminal, (target_sequence, new_characters))] = (state, (target_sequence, new_characters))\n",
    "            continue\n",
    "        if terminal in parser.lexer_conf.ignore:\n",
    "            to_test = target_sequence\n",
    "        elif terminal == \"\":\n",
    "            if target_sequence[0] not in next_potential_terminals[(\"file_input\", \"variable_start\")]:\n",
    "                configuration_map2[(terminal, (target_sequence, new_characters))] = None\n",
    "                continue\n",
    "            else:\n",
    "                to_test = target_sequence\n",
    "        else:\n",
    "            to_test = (terminal,) + target_sequence\n",
    "        for i in range(len(to_test) - 1):\n",
    "            if to_test[i+1] not in next_potential_terminals[(to_test[i], \"terminal\")]:\n",
    "                configuration_map2[(terminal, (target_sequence, new_characters))] = None\n",
    "                break\n",
    "        else:\n",
    "            configuration_map2[(terminal, (target_sequence, new_characters))] = (terminal, (target_sequence, new_characters))\n",
    "\n",
    "    # Use the always legal (terminal, [terminal2]) configurations to merge equivalent configurations\n",
    "    # Use the never legal (terminal, [terminal2]) configurations to remove impossible configurations\n",
    "    \n",
    "    pairs_of_terminals = defaultdict(set)\n",
    "    for x in terminals:\n",
    "        terminal = terminal_replacement[x.value]\n",
    "        for terminal2 in next_potential_terminals[(terminal, \"terminal\")]:\n",
    "            terminal2 = terminal_replacement[terminal2]\n",
    "            pairs_of_terminals[accepts(terminal, (terminal2,))].add((terminal, terminal2))\n",
    "    \n",
    "    for x in set(configuration_map2.values()):\n",
    "        if x is None:\n",
    "            continue\n",
    "        (terminal, (target_sequence, new_characters)) = x\n",
    "        if len(target_sequence) == 0:\n",
    "            configuration_map3[(terminal, (target_sequence, new_characters))] = (terminal, (target_sequence, new_characters))\n",
    "            continue\n",
    "        if terminal in parser.lexer_conf.ignore or terminal == \"\":\n",
    "            to_test = target_sequence\n",
    "        else:\n",
    "            to_test = (terminal,) + target_sequence\n",
    "        if len(to_test) == 1:\n",
    "            configuration_map3[(terminal, (target_sequence, new_characters))] = (terminal, (target_sequence, new_characters))\n",
    "            continue\n",
    "        for i in range(len(to_test) - 1):\n",
    "            if (to_test[i], to_test[i+1]) in pairs_of_terminals[\"never legal\"]:\n",
    "                configuration_map3[(terminal, target_sequence)] = None\n",
    "                continue\n",
    "        i = len(to_test) - 2\n",
    "        while True:\n",
    "            if (to_test[i], to_test[i+1]) in pairs_of_terminals[\"always legal\"]:\n",
    "                to_test = to_test[:i+1]\n",
    "            else:\n",
    "                break\n",
    "            i = i - 1\n",
    "            if i < 0:\n",
    "                break\n",
    "        if terminal in parser.lexer_conf.ignore or terminal == \"\":\n",
    "            configuration_map3[(terminal, (target_sequence, new_characters))] = (terminal, (to_test, new_characters))\n",
    "        else:\n",
    "            configuration_map3[(terminal, (target_sequence, new_characters))] = (terminal, (to_test[1:], new_characters))\n",
    "\n",
    "    # Remove \"always legal\" and \"never legal\" configurations\n",
    "    possibly_always_legal = set()\n",
    "    i = 0\n",
    "    counter = defaultdict(int)\n",
    "    for x in set(configuration_map3.values()):\n",
    "        if x is None:\n",
    "            continue\n",
    "        (terminal, (target_sequence, new_characters)) = x\n",
    "        i += 1\n",
    "        if len(target_sequence) == 0:\n",
    "            configuration_map4[(terminal, (target_sequence, new_characters))] = (terminal, (target_sequence, new_characters))\n",
    "            continue\n",
    "        if terminal == \"\":\n",
    "            accepted = accepts(\"file_input\", target_sequence, terminal=False, maximum_delta=2)\n",
    "        elif terminal in parser.lexer_conf.ignore:\n",
    "            if len(target_sequence) == 1:\n",
    "                configuration_map4[(terminal, (target_sequence, new_characters))] = (terminal, (target_sequence, new_characters))\n",
    "                continue\n",
    "            accepted = accepts(target_sequence[0], target_sequence[1:], maximum_delta=2)\n",
    "        else:\n",
    "            accepted = accepts(terminal, target_sequence, maximum_delta=2)\n",
    "        counter[accepted] += 1\n",
    "        if accepted == \"never legal\":\n",
    "            configuration_map4[(terminal, (target_sequence, new_characters))] = None\n",
    "            continue\n",
    "        elif accepted == \"always legal\":\n",
    "            if terminal in parser.lexer_conf.ignore:\n",
    "                configuration_map4[(terminal, (target_sequence, new_characters))] = (terminal, (target_sequence[:1], new_characters))\n",
    "            else:\n",
    "                configuration_map4[(terminal, (target_sequence, new_characters))] = (terminal, ((), new_characters))\n",
    "        elif accepted == \"possibly always legal\":\n",
    "            configuration_map4[(terminal, (target_sequence, new_characters))] = (terminal, (target_sequence, new_characters))\n",
    "            possibly_always_legal.add((terminal, (target_sequence, new_characters)))\n",
    "        elif accepted == \"sometimes legal\":\n",
    "            configuration_map4[(terminal, (target_sequence, new_characters))] = (terminal, (target_sequence, new_characters))\n",
    "        elif accepted == \"possibly never legal\":\n",
    "            if not is_never_legal(terminal, list(target_sequence)):\n",
    "                configuration_map4[(terminal, (target_sequence, new_characters))] = (terminal, (target_sequence, new_characters))\n",
    "                possibly_always_legal.add((terminal, (target_sequence, new_characters)))\n",
    "            else:\n",
    "                counter[\"confirmed never legal\"] += 1\n",
    "                configuration_map4[(terminal, (target_sequence, new_characters))] = None\n",
    "                continue\n",
    "\n",
    "    with open(\"configuration_maps.pkl\", \"wb\") as f:\n",
    "        pickle.dump([\n",
    "            configuration_map,\n",
    "            configuration_map2,\n",
    "            configuration_map3,\n",
    "            configuration_map4,\n",
    "            possibly_always_legal\n",
    "        ], f)\n",
    "\n",
    "print(\n",
    "    len(configuration_map.keys()),\n",
    "    len(set(configuration_map.values())),\n",
    "    len(set(configuration_map2.values())),\n",
    "    len(set(configuration_map3.values())),\n",
    "    len(set(configuration_map4.values()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_streamlined_mask(translation_map):\n",
    "    token_nfa.streamlined_mask = {}\n",
    "    for s in token_nfa.mask:\n",
    "        terminal = state2terminal[s]\n",
    "        token_nfa.streamlined_mask[s] = {}\n",
    "        for path in token_nfa.mask[s]:\n",
    "            if path is None:\n",
    "                token_nfa.streamlined_mask[s][path] = token_nfa.mask[s][path]\n",
    "            else:\n",
    "                new_path = translation_map[(terminal, path)]\n",
    "                if new_path is None:\n",
    "                    continue\n",
    "                new_path = new_path[1]\n",
    "                if new_path not in token_nfa.streamlined_mask[s]:\n",
    "                    token_nfa.streamlined_mask[s][new_path] = token_nfa.mask[s][path]\n",
    "                else:\n",
    "                    token_nfa.streamlined_mask[s][new_path] = torch.bitwise_or(\n",
    "                        token_nfa.streamlined_mask[s][new_path],\n",
    "                        token_nfa.mask[s][path]\n",
    "                    )\n",
    "\n",
    "translation_map = {}\n",
    "for k in configuration_map:\n",
    "    k2 = configuration_map[k]\n",
    "    translation_map[k] = k2\n",
    "\n",
    "add_streamlined_mask(translation_map)    \n",
    "x1 = [len(token_nfa.streamlined_mask[s]) for s in token_nfa.streamlined_mask]\n",
    "\n",
    "for k in configuration_map:\n",
    "    k2 = configuration_map[k]\n",
    "    if k2 is None:\n",
    "        translation_map[k] = None\n",
    "        continue\n",
    "    k3 = configuration_map2[k2]\n",
    "    translation_map[k] = k3\n",
    "\n",
    "add_streamlined_mask(translation_map)\n",
    "x2 = [len(token_nfa.streamlined_mask[s]) for s in token_nfa.streamlined_mask]\n",
    "    \n",
    "for k in configuration_map:\n",
    "    k2 = configuration_map[k]\n",
    "    if k2 is None:\n",
    "        translation_map[k] = None\n",
    "        continue\n",
    "    k3 = configuration_map2[k2]\n",
    "    if k3 is None:\n",
    "        translation_map[k] = None\n",
    "        continue\n",
    "    k4 = configuration_map3[k3]\n",
    "    translation_map[k] = k4\n",
    "\n",
    "add_streamlined_mask(translation_map)\n",
    "x3 = [len(token_nfa.streamlined_mask[s]) for s in token_nfa.streamlined_mask]\n",
    "\n",
    "for k in configuration_map:\n",
    "    k2 = configuration_map[k]\n",
    "    if k2 is None:\n",
    "        translation_map[k] = None\n",
    "        continue\n",
    "    k3 = configuration_map2[k2]\n",
    "    if k3 is None:\n",
    "        translation_map[k] = None\n",
    "        continue\n",
    "    k4 = configuration_map3[k3]\n",
    "    if k4 is None:\n",
    "        translation_map[k] = None       \n",
    "        \n",
    "    k4 = configuration_map3[k3]\n",
    "    if k4 is None:\n",
    "        translation_map[k] = None\n",
    "        continue\n",
    "    k5 = configuration_map4[k4]\n",
    "    translation_map[k] = k5\n",
    "\n",
    "add_streamlined_mask(translation_map)\n",
    "x4 = [len(token_nfa.streamlined_mask[s]) for s in token_nfa.streamlined_mask]\n",
    "sum([len(token_nfa.mask[s]) for s in token_nfa.mask]), sum(x1), sum(x2), sum(x3), sum(x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEmeluZBHS9C"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "We tokenize a series of Python files from four GitHub repositories (more than one million tokens in total) and for each token, we compute the masks thanks to the streamlined mask store and we check that the next token is indeed allowed by the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FlVcfBnLE592"
   },
   "outputs": [],
   "source": [
    "if \"python_files\" not in os.listdir():\n",
    "    import autopep8\n",
    "    import sys\n",
    "\n",
    "    def is_ascii(s):\n",
    "        try:\n",
    "            s.encode('ascii')\n",
    "        except UnicodeEncodeError:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def normalize_file_indentation(root, filename, target):\n",
    "        # Read the content of the file\n",
    "        with open(os.path.join(root, filename), 'r') as file:\n",
    "            original_content = file.read()\n",
    "\n",
    "        if not is_ascii(original_content):\n",
    "            print(f\"{filename} skipped (because it contains non-ASCII characters)\")\n",
    "        else:\n",
    "            # Use autopep8 to format the file content\n",
    "            formatted_content = autopep8.fix_code(original_content, options={\n",
    "                'indent_size': 4,\n",
    "                'aggressive': 2,\n",
    "                'experimental': True\n",
    "            })\n",
    "\n",
    "            # Write the formatted content back to the file\n",
    "            with open(os.path.join(target, root.replace(\"/\", \"-\")+\"-\"+filename), 'w') as file:\n",
    "                file.write(formatted_content)\n",
    "                print(f\"{filename} processed with autopep8\")\n",
    "\n",
    "    !git clone https://github.com/uiuc-focal-lab/syncode.git\n",
    "    !git clone https://github.com/sgl-project/sglang.git\n",
    "    !git clone https://github.com/dottxt-ai/outlines.git\n",
    "    !git clone https://github.com/eth-sri/lmql.git\n",
    "    os.mkdir(\"python_files\")\n",
    "    for repo in [\"syncode\", \"sglang\", \"outlines\", \"lmql\"]:\n",
    "        for root, subdirs, files in os.walk(repo):\n",
    "            for f in files:\n",
    "                if f.endswith(\".py\"):\n",
    "                    normalize_file_indentation(root, f, \"python_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Nbe765bsFD05",
    "outputId": "3a8cdb9f-94c8-4dd1-cf05-12d00229cec7"
   },
   "outputs": [],
   "source": [
    "streamlined = True\n",
    "if False:#\"already_checked.json\" in os.listdir():\n",
    "    already_checked = json.load(open(\"already_checked.json\", \"r\"))\n",
    "else:\n",
    "    already_checked = {}\n",
    "\n",
    "for f in os.listdir(\"python_files\"):\n",
    "    if f in already_checked and type(already_checked[f]) == int:\n",
    "        continue\n",
    "    with open(os.path.join(\"python_files\", f), \"r\") as file:\n",
    "        prompt = file.read()\n",
    "        token_ids = tokenizer(prompt)[\"input_ids\"][1:]\n",
    "        \n",
    "        # Skip the file if it contains some characters: `\\t`, `\\r`, `\\f`\n",
    "        for c in prompt:\n",
    "            if c in [\"\\t\", \"\\r\", \"\\f\"]:\n",
    "                already_checked[f] = 0\n",
    "                json.dump(already_checked, open(\"already_checked.json\", \"w\"))\n",
    "                break\n",
    "        if f in already_checked:\n",
    "            continue\n",
    "        \n",
    "        # Skip the file if it isn't synctatically correct given the Lark Python grammar\n",
    "        try:\n",
    "            parser.parse(prompt)\n",
    "        except:\n",
    "            already_checked[f] = 0\n",
    "            json.dump(already_checked, open(\"already_checked.json\", \"w\"))\n",
    "            continue\n",
    "        \n",
    "        mask_generator = PythonMaskGenerator(parser, token_nfa)\n",
    "        mask = mask_generator.build_mask()\n",
    "        i, start = 0, time.time()\n",
    "        \n",
    "        for token_id in token_ids:\n",
    "            # Fail if the mask is excessively restrictive\n",
    "            if not mask[token_id]:\n",
    "                already_checked[f] = (f, i, token_id, get_token(token_id, tokenizer), mask_generator.states)\n",
    "                print(\"******\", *already_checked[f])\n",
    "                json.dump(already_checked, open(\"already_checked.json\", \"w\"))\n",
    "                break\n",
    "            try:\n",
    "                # Consume the next token id and update the mask\n",
    "                mask_generator.consume(token_id)\n",
    "                mask = mask_generator.build_mask(streamlined=streamlined)\n",
    "            # Fail if the next terminal is not accepted by the parser\n",
    "            except lark.UnexpectedToken:\n",
    "                already_checked[f] = (f, i, token_id, get_token(token_id, tokenizer), mask_generator.states)\n",
    "                print(\"******\", *already_checked[f])\n",
    "                json.dump(already_checked, open(\"already_checked.json\", \"w\"))\n",
    "            i += 1\n",
    "        else:\n",
    "            try:\n",
    "                mask_generator.terminate()\n",
    "                already_checked[f] = len(token_ids)\n",
    "                json.dump(already_checked, open(\"already_checked.json\", \"w\"))\n",
    "            except:\n",
    "                already_checked[f] = (f, i, token_id, get_token(token_id, tokenizer), mask_generator.states)\n",
    "                print(\"******\", *already_checked[f])\n",
    "                json.dump(already_checked, open(\"already_checked.json\", \"w\"))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(already_checked[f] for f in already_checked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "We check that strings (more than one million characters in total) generated using the streamlined mask store are syntatically correct Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing_outcome = {}\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    np.random.seed(seed=i)\n",
    "    token_ids = []\n",
    "    mask_generator = PythonMaskGenerator(parser, token_nfa)\n",
    "    try:\n",
    "        for _ in range(1000):\n",
    "            mask = mask_generator.build_mask(streamlined=True)\n",
    "            token_id = np.argmax(np.random.rand(32000)*np.array(mask))\n",
    "            mask_generator.consume(token_id)\n",
    "            token_ids.append(token_id)     \n",
    "        \n",
    "        prompt = tokenizer.decode(token_ids)\n",
    "        length_unparsed = len(\"\".join(mask_generator.states[0][2]))\n",
    "        parser.parse_interactive(prompt[:-length_unparsed]).exhaust_lexer()\n",
    "        parsing_outcome[i] = len(prompt) - length_unparsed\n",
    "        json.dump(parsing_outcome, open(\"infinite_monkey.json\", \"w\"))\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            num_characters_validated = sum(parsing_outcome[x] for x in parsing_outcome if type(parsing_outcome[x]) == int)\n",
    "            print(i, num_characters_validated)\n",
    "            if num_characters_validated > 1_000_000:\n",
    "                break\n",
    "    except:\n",
    "        parsing_outcome[i] = \"\"\n",
    "        json.dump(parsing_outcome, open(\"infinite_monkey.json\", \"w\"))\n",
    "        print(i, tokenizer.decode(token_ids))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3\n",
    "\n",
    "For more than 100,000 tokens extracted from the same Python files as before, we compute the masks using both the original mask store and the streamlined mask store and we check that the resulting masks were systematically identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 0\n",
    "for f in os.listdir(\"python_files\"):\n",
    "    if already_checked[f] > 0:\n",
    "        with open(os.path.join(\"python_files\", f), \"r\") as file:\n",
    "            prompt = file.read()\n",
    "            token_ids = tokenizer(prompt)[\"input_ids\"][1:]   \n",
    "\n",
    "        mask_generator = PythonMaskGenerator(parser, token_nfa)\n",
    "        mask = mask_generator.build_mask(streamlined=True)\n",
    "        mask2 = mask_generator.build_mask(streamlined=False)\n",
    "        assert mask.equal(mask2)\n",
    "        \n",
    "        for token_id in token_ids:\n",
    "            assert mask[token_id]\n",
    "            assert mask2[token_id]\n",
    "            mask_generator.consume(token_id)\n",
    "            mask = mask_generator.build_mask(streamlined=True)\n",
    "            mask2 = mask_generator.build_mask(streamlined=False)\n",
    "            assert mask.equal(mask2)\n",
    "\n",
    "    num_tokens += already_checked[f]\n",
    "    if num_tokens > 100_000:\n",
    "        print(f\"{num_tokens} tokens read. No discrepancy between the mask store and the streamlined mask store\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "B3eeDlsVn8jz",
    "0dkE8quWoDzF",
    "sVQTFtymoGsL",
    "DyN7cECdFga7",
    "XebFq-awx5A8",
    "v-xl5ZyEqrD_"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "createdOn": 1732607465853,
  "creator": "admin",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env py_310_constrained_decoding)",
   "language": "python",
   "name": "py-dku-venv-py_310_constrained_decoding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "modifiedBy": "admin",
  "tags": [],
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07e085b049df40759fcc501206fe6d38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "layout": "IPY_MODEL_6968b3ebef17477f98e7671c6e77c13c",
      "placeholder": "​",
      "style": "IPY_MODEL_322ca904cad5403c8ff1555b10a1daf9",
      "value": " 996/996 [00:00&lt;00:00, 22.6kB/s]"
     }
    },
    "08b990d44bdb47ecb51eff0d3b442bf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "17799bfa4c0d45a793f278326a06405c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20a3879179384546888366c51c9fb629": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "2c3a889ec0cf4ef1b13eea811d736465": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2dba1fb04aa04ca09bd7c09f22401511",
       "IPY_MODEL_cb78171d32514589b740ca711d1a275e",
       "IPY_MODEL_51d53428f2004238975b5607550dbdd8"
      ],
      "layout": "IPY_MODEL_7ec2033100874395aa0fc9cbf822779c"
     }
    },
    "2dba1fb04aa04ca09bd7c09f22401511": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "layout": "IPY_MODEL_4af153566d4c44d8a66437feb777b696",
      "placeholder": "​",
      "style": "IPY_MODEL_da31d7ccc21a49e58d67657a90198e75",
      "value": "tokenizer.model: 100%"
     }
    },
    "322ca904cad5403c8ff1555b10a1daf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "355ad2a5c97e453b9d456a99adb00fda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3660effca87e486ab420ffdfe78c35cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "391ea20aa58d4f739e8b1ad63c8eed79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "3b464f0d253341d58674803ecbd4ba28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "layout": "IPY_MODEL_bc0378cfe8b24813949b1a6cea57844b",
      "max": 414,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_72aa012d15a444d68602da1fe55978af",
      "value": 414
     }
    },
    "3dc769136b534ecf93142ae7e0ec7ec0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "46634823d9d14de18264eba6175f8307": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "layout": "IPY_MODEL_8b0f358f67714d138637327ce0728c14",
      "placeholder": "​",
      "style": "IPY_MODEL_3dc769136b534ecf93142ae7e0ec7ec0",
      "value": " 1.80M/1.80M [00:00&lt;00:00, 4.24MB/s]"
     }
    },
    "4af153566d4c44d8a66437feb777b696": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "51d53428f2004238975b5607550dbdd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "layout": "IPY_MODEL_72e322611d8049a4b916148f1f2459b9",
      "placeholder": "​",
      "style": "IPY_MODEL_08b990d44bdb47ecb51eff0d3b442bf3",
      "value": " 493k/493k [00:00&lt;00:00, 5.92MB/s]"
     }
    },
    "55a8ce618602446f9f72e3b2161467a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63ef3900544f4ee29050d0a47acd7819": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "6968b3ebef17477f98e7671c6e77c13c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "6a23b1837fac4d8d9fd1a33abdeb7643": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8cb8f7f4ae5648df9a1f50c15d90c9ec",
       "IPY_MODEL_3b464f0d253341d58674803ecbd4ba28",
       "IPY_MODEL_922679c1d4be478aaef7b20f4c5bb522"
      ],
      "layout": "IPY_MODEL_20a3879179384546888366c51c9fb629"
     }
    },
    "6c73536c2b334371bdabd736bed98802": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "72aa012d15a444d68602da1fe55978af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72e322611d8049a4b916148f1f2459b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "75d20873f81443f2b25cb7c4b03dc98e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "layout": "IPY_MODEL_936defb9ad4a4157b703679ea4b10e0c",
      "max": 996,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17799bfa4c0d45a793f278326a06405c",
      "value": 996
     }
    },
    "7ec2033100874395aa0fc9cbf822779c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "7fb46faf07f14dcc8a6d1106ad3f3604": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "layout": "IPY_MODEL_6c73536c2b334371bdabd736bed98802",
      "placeholder": "​",
      "style": "IPY_MODEL_3660effca87e486ab420ffdfe78c35cb",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "8ad2fe47632447d0aae446dbe7fa6bf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "8b0f358f67714d138637327ce0728c14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "8cb8f7f4ae5648df9a1f50c15d90c9ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "layout": "IPY_MODEL_391ea20aa58d4f739e8b1ad63c8eed79",
      "placeholder": "​",
      "style": "IPY_MODEL_55a8ce618602446f9f72e3b2161467a0",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "90b07bad47b44a378f8f5d4e7299d452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "layout": "IPY_MODEL_63ef3900544f4ee29050d0a47acd7819",
      "max": 1795188,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc3d4feaabb44a7ba89ef277724b932a",
      "value": 1795188
     }
    },
    "922679c1d4be478aaef7b20f4c5bb522": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "layout": "IPY_MODEL_a95c7dbf6697497a874449cfb471b4de",
      "placeholder": "​",
      "style": "IPY_MODEL_e245e4337a074ca0a1fc54f5d2c9c803",
      "value": " 414/414 [00:00&lt;00:00, 9.37kB/s]"
     }
    },
    "936defb9ad4a4157b703679ea4b10e0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "a2f48961ad3c44aea4ba8a753da6afab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "layout": "IPY_MODEL_8ad2fe47632447d0aae446dbe7fa6bf8",
      "placeholder": "​",
      "style": "IPY_MODEL_d6e327ad2a3c43d2b031062a287b9b21",
      "value": "tokenizer.json: 100%"
     }
    },
    "a95c7dbf6697497a874449cfb471b4de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "bc0378cfe8b24813949b1a6cea57844b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "cb78171d32514589b740ca711d1a275e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "layout": "IPY_MODEL_db5d2e9af0ec45749a426e3a746f4837",
      "max": 493443,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_355ad2a5c97e453b9d456a99adb00fda",
      "value": 493443
     }
    },
    "d6e327ad2a3c43d2b031062a287b9b21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7c9a06cf4744ee2a4515a85417295b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "da31d7ccc21a49e58d67657a90198e75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db5d2e9af0ec45749a426e3a746f4837": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "e245e4337a074ca0a1fc54f5d2c9c803": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ecb666e96f314cccbb323abb7a80bce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView"
     }
    },
    "f2ef4327e4024bf2935f1ed135b03de5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2f48961ad3c44aea4ba8a753da6afab",
       "IPY_MODEL_90b07bad47b44a378f8f5d4e7299d452",
       "IPY_MODEL_46634823d9d14de18264eba6175f8307"
      ],
      "layout": "IPY_MODEL_ecb666e96f314cccbb323abb7a80bce7"
     }
    },
    "fb51c671b2cc4bd0a830f17700a1847a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7fb46faf07f14dcc8a6d1106ad3f3604",
       "IPY_MODEL_75d20873f81443f2b25cb7c4b03dc98e",
       "IPY_MODEL_07e085b049df40759fcc501206fe6d38"
      ],
      "layout": "IPY_MODEL_d7c9a06cf4744ee2a4515a85417295b5"
     }
    },
    "fc3d4feaabb44a7ba89ef277724b932a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
